{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d33c95b",
   "metadata": {},
   "source": [
    "간단한 함수의 예를 하나 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706fe0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7220b4",
   "metadata": {},
   "source": [
    "신경망은 보통 수만 개의 파라미터를 가진 매우 복잡한 함수입니다.<br>\n",
    "손으로 직접 도함수를 계산하는 것은 거의 불가능합니다.\n",
    "\n",
    "아래 코드는 한 가지 대안입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a709851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e921a4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1 + eps, w2) - f(w1, w2)) / eps  # (5, 3)에서 w1에 대한 도함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b989b22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps  # (5, 3)에서 w2에 대한 도함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63dc6e",
   "metadata": {},
   "source": [
    "이 방법은 잘 동작하고 구현하기도 쉽습니다.<br>\n",
    "하지만 근삿값이고 무엇보다도 파라미터마다 적어도 한 번씩은 함수 f()를 호출해야 하므로<br>\n",
    "대규모 신경망에서는 적용하기 어렵습니다.\n",
    "\n",
    "대신 자동 미분을 써봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6e04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "# 아래 코드에서 이 변수와 관련된 모든 연산을 자동으로 기록합니다.\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "# tape에 두 변수 [w1, w2]에 대한 z의 그레이디언트를 요청합니다.\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c016275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071f1fa",
   "metadata": {},
   "source": [
    "결과가 정확하고 변수가 얼마나 많든지 gradient 메소드는 기록된 계산을 한 번 만에 통과했습니다.<br>\n",
    "매우 효율적인 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6435b6",
   "metadata": {},
   "source": [
    "gradient() 메소드가 호출된 후에는 자동으로 테이프가 즉시 지워집니다.<br>\n",
    "따라서 gradient() 메소드를 두 번 호출하면 예외가 발생합니다.\n",
    "\n",
    "gradient() 메소드를 계속 호출해야 한다면 지속 가능한 테이프를 만들고 사용이 끝난 후 테이프를 삭제해서 리소스를 해제해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44eeea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4238e41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=36.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70873d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac97ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ac116",
   "metadata": {},
   "source": [
    "기본적으로 테이프는 변수가 포함된 연산만을 기록합니다.<br>\n",
    "만약 변수가 아닌 다른 객체에 대한 z의 그레이디언트를 계산하려 하면 None이 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddff0d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a1173",
   "metadata": {},
   "source": [
    "변수와 상수가 섞여있다면, 변수에 대한 그레이디언트만 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06b354c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, v2 = tf.constant(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, v2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, v2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad000762",
   "metadata": {},
   "source": [
    "하지만 필요한 어떤 텐서라도 감시하여 관련된 모든 연산을 기록하도록 강제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ff5377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f36f0",
   "metadata": {},
   "source": [
    "어떤 경우에는 신경망의 일부분에 그레이디언트가 역전파되지 않도록 막을 필요가 있습니다.<br>\n",
    "이렇게 하려면 tf.stop_gradient() 함수를 사용해야 합니다.\n",
    "\n",
    "이 함수는 정방향 계산을 할 때는 입력을 반환하고,<br>\n",
    "역전파 시에는 그레이디언트를 전파하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "027421fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)  # stop_gradient()가 없을 때와 결과가 같습니다.\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcab4a",
   "metadata": {},
   "source": [
    "이따금 그레이디언트를 계산할 때 수치적 이슈가 발생할 수 있습니다.<br>\n",
    "예를 들면 큰 입력에 대한 my_softplus() 함수의 그레이디언트를 계산하면 NaN이 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "822dc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): # tf.nn.softplus(z) 값을 반환합니다\n",
    "    return tf.math.log(tf.exp(z) + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87eba8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc0ca8",
   "metadata": {},
   "source": [
    "자동 미분을 사용해 이 함수의 그레이디언트를 계산하는 게 수치적으로 불안정하기 때문입니다.<br>\n",
    "\n",
    "다행히 이 경우 softplus의 도함수를 해석적으로 구할 수 있습니다.<br>\n",
    "@tf.custom_gradient 데코레이터를 사용하고<br>\n",
    "일반 출력과 도함수를 계산하는 함수를 반환하여 텐서플로가 my_softplus() 함수의 그레이디언트를 계산할 때 안전한 함수를 사용하게 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d60689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
